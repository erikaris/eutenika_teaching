{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Sentiment analysis is the interpretation and classification of emotions (positive, negative and neutral) within text data using text analysis techniques. Before to do it, we have to make sure to install some required packages ```pip install pandas nltk sklearn Sastrawi```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before creating the model, we have to supervise the tweets sample, and provide the sentiment manually."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Tweet</th>\n",
       "      <th>sentimen</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>rt @napqilla: no 1, 3 ambisinya menguasai raky...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>rt @pandji: nah gue pikir sentimen petahana ok...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>rt @pandji: urutan pertama best moment #debat2...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>rt @pandji: ini artikel yg menjelaskan ternyat...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>rt @mrtampi: agus makin santai.\\nahok makin sa...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1501</th>\n",
       "      <td>rt @erixputra: rakyat adalah bos kami. kami ad...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1502</th>\n",
       "      <td>ahok \\u2013 djarot waspadai politik uang dalam...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1503</th>\n",
       "      <td>@jokowi harusnya sdh tahu dari awal ini semaki...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1504</th>\n",
       "      <td>#coblosnomor2 soal transportasi, ahok sebut an...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1505</th>\n",
       "      <td>rt @tanpadelusi: guys, jangan lupa besok hari ...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1506 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                  Tweet  sentimen\n",
       "0     rt @napqilla: no 1, 3 ambisinya menguasai raky...         1\n",
       "1     rt @pandji: nah gue pikir sentimen petahana ok...         1\n",
       "2     rt @pandji: urutan pertama best moment #debat2...         1\n",
       "3     rt @pandji: ini artikel yg menjelaskan ternyat...         1\n",
       "4     rt @mrtampi: agus makin santai.\\nahok makin sa...         0\n",
       "...                                                 ...       ...\n",
       "1501  rt @erixputra: rakyat adalah bos kami. kami ad...         1\n",
       "1502  ahok \\u2013 djarot waspadai politik uang dalam...         1\n",
       "1503  @jokowi harusnya sdh tahu dari awal ini semaki...         1\n",
       "1504  #coblosnomor2 soal transportasi, ahok sebut an...         0\n",
       "1505  rt @tanpadelusi: guys, jangan lupa besok hari ...         0\n",
       "\n",
       "[1506 rows x 2 columns]"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "review_data = pd.read_csv('training_all_random.csv', sep=';', names=['Tweet', 'sentimen'])\n",
    "review_data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next step is tokenize each tweet. Tokenize is a process dividing a sentence into several tokens (can be words or others depends on context)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Tweet</th>\n",
       "      <th>sentimen</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>rt napqilla no ambisinya menguasai rakyat no a...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>rt pandji nah gue pikir sentimen petahana oke ...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>rt pandji urutan pertama best moment debat pil...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>rt pandji ini artikel yg menjelaskan ternyata ...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>rt mrtampi agus makin santai nahok makin santu...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1501</th>\n",
       "      <td>rt erixputra rakyat adalah bos kami kami adala...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1502</th>\n",
       "      <td>ahok u djarot waspadai politik uang dalam pilk...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1503</th>\n",
       "      <td>jokowi harusnya sdh tahu dari awal ini semakin...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1504</th>\n",
       "      <td>coblosnomor soal transportasi ahok sebut anies...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1505</th>\n",
       "      <td>rt tanpadelusi guys jangan lupa besok hari sid...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1506 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                  Tweet  sentimen\n",
       "0     rt napqilla no ambisinya menguasai rakyat no a...         1\n",
       "1     rt pandji nah gue pikir sentimen petahana oke ...         1\n",
       "2     rt pandji urutan pertama best moment debat pil...         1\n",
       "3     rt pandji ini artikel yg menjelaskan ternyata ...         1\n",
       "4     rt mrtampi agus makin santai nahok makin santu...         0\n",
       "...                                                 ...       ...\n",
       "1501  rt erixputra rakyat adalah bos kami kami adala...         1\n",
       "1502  ahok u djarot waspadai politik uang dalam pilk...         1\n",
       "1503  jokowi harusnya sdh tahu dari awal ini semakin...         1\n",
       "1504  coblosnomor soal transportasi ahok sebut anies...         0\n",
       "1505  rt tanpadelusi guys jangan lupa besok hari sid...         0\n",
       "\n",
       "[1506 rows x 2 columns]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from nltk.tokenize import RegexpTokenizer\n",
    "\n",
    "tokenizer = RegexpTokenizer(r'[A-Za-z]+')\n",
    "\n",
    "review_data['Tweet'] = review_data.Tweet.map(lambda x: tokenizer.tokenize(x))\n",
    "review_data.Tweet = review_data.Tweet.str.join(sep=' ')\n",
    "review_data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next step is remove stop words. Stop words are natural language words which have very little meaning, such as \"and\", \"the\", \"a\", \"an\", and similar words. In indonesian language, some common stop words includes “yang”, “untuk”, “pada”, “ke”, etc."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Tweet</th>\n",
       "      <th>sentimen</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>rt napqilla no ambisinya menguasai rakyat no a...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>rt pandji nah gue pikir sentimen petahana oke ...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>rt pandji urutan pertama best moment debat pil...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>rt pandji artikel yg menjelaskan ternyata deba...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>rt mrtampi agus makin santai nahok makin santu...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1501</th>\n",
       "      <td>rt erixputra rakyat bos kami pelayan rakyat u ...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1502</th>\n",
       "      <td>ahok u djarot waspadai politik uang pilkada ht...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1503</th>\n",
       "      <td>jokowi harusnya sdh tahu awal semakin masif hi...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1504</th>\n",
       "      <td>coblosnomor soal transportasi ahok sebut anies...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1505</th>\n",
       "      <td>rt tanpadelusi guys jangan lupa besok hari sid...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1506 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                  Tweet  sentimen\n",
       "0     rt napqilla no ambisinya menguasai rakyat no a...         1\n",
       "1     rt pandji nah gue pikir sentimen petahana oke ...         1\n",
       "2     rt pandji urutan pertama best moment debat pil...         1\n",
       "3     rt pandji artikel yg menjelaskan ternyata deba...         1\n",
       "4     rt mrtampi agus makin santai nahok makin santu...         0\n",
       "...                                                 ...       ...\n",
       "1501  rt erixputra rakyat bos kami pelayan rakyat u ...         1\n",
       "1502  ahok u djarot waspadai politik uang pilkada ht...         1\n",
       "1503  jokowi harusnya sdh tahu awal semakin masif hi...         1\n",
       "1504  coblosnomor soal transportasi ahok sebut anies...         0\n",
       "1505  rt tanpadelusi guys jangan lupa besok hari sid...         0\n",
       "\n",
       "[1506 rows x 2 columns]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from Sastrawi.StopWordRemover.StopWordRemoverFactory import StopWordRemoverFactory\n",
    "\n",
    "stopword = StopWordRemoverFactory().create_stop_word_remover()\n",
    "\n",
    "review_data['Tweet'] = review_data.Tweet.map(lambda x: stopword.remove(x)) \n",
    "review_data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next step is stemming. Stemming is the process of producing morphological variants of a root/base word. In english language, there are several popular algorithms, including Potter’s, Lovins, Dawson, Krovetz, etc. While in indonesia, there are library called Sastrawi."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Its very slow comparing English Stemmer. So it will be disabled\n",
    "\n",
    "# from multiprocessing.dummy import Pool as ThreadPool\n",
    "# from Sastrawi.Stemmer.StemmerFactory import StemmerFactory\n",
    "\n",
    "# def stem(x):\n",
    "#     stemmer = StemmerFactory().create_stemmer()\n",
    "#     return stemmer.stem(x)\n",
    "\n",
    "# pool = ThreadPool(4)\n",
    "# review_data['Tweet'] = pool.map(stem, review_data.Tweet)\n",
    "# pool.close()\n",
    "# pool.join()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next step is creating bag of words with CountVectorizer. With CountVectorizer we are converting raw text to a numerical vector representation of words and n-grams. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['aa',\n",
       " 'aaakysdoke',\n",
       " 'aagym',\n",
       " 'aamandakasih',\n",
       " 'aamiin',\n",
       " 'aangku',\n",
       " 'abah',\n",
       " 'abaikan',\n",
       " 'abal',\n",
       " 'abdee',\n",
       " 'abdusshomad',\n",
       " 'abiiiis',\n",
       " 'about',\n",
       " 'abs',\n",
       " 'absolutely',\n",
       " 'abstrak',\n",
       " 'academia',\n",
       " 'acara',\n",
       " 'acded',\n",
       " 'aceh',\n",
       " 'acehcenterid',\n",
       " 'acg',\n",
       " 'acta',\n",
       " 'acung',\n",
       " 'ad',\n",
       " 'ada',\n",
       " 'adakah',\n",
       " 'adam',\n",
       " 'addiems',\n",
       " 'adhdhaifah',\n",
       " 'adib',\n",
       " 'adibhidayat',\n",
       " 'adil',\n",
       " 'adj',\n",
       " 'adl',\n",
       " 'adlh',\n",
       " 'adsaimugbf',\n",
       " 'adu',\n",
       " 'aduh',\n",
       " 'ae',\n",
       " 'aef',\n",
       " 'afb',\n",
       " 'afhbfellzu',\n",
       " 'afif',\n",
       " 'afiit',\n",
       " 'afnr',\n",
       " 'afqzzgsz',\n",
       " 'afu',\n",
       " 'aga',\n",
       " 'agama',\n",
       " 'agamanya',\n",
       " 'agamis',\n",
       " 'agatha',\n",
       " 'agitasi',\n",
       " 'agkgoerfpz',\n",
       " 'agree',\n",
       " 'agrveu',\n",
       " 'agungwidrajat',\n",
       " 'agus',\n",
       " 'agusfansclub',\n",
       " 'agussylvi',\n",
       " 'agussylvidki',\n",
       " 'agusyudhoyono',\n",
       " 'ah',\n",
       " 'ahaaayeee',\n",
       " 'ahhhh',\n",
       " 'ahli',\n",
       " 'ahok',\n",
       " 'ahokbersinetron',\n",
       " 'ahokdipecat',\n",
       " 'ahokdjarot',\n",
       " 'ahokdjarothebat',\n",
       " 'ahokdjarotpilihan',\n",
       " 'ahoker',\n",
       " 'ahokers',\n",
       " 'ahoklah',\n",
       " 'ahokoke',\n",
       " 'ahuv',\n",
       " 'ahy',\n",
       " 'ahydki',\n",
       " 'ahymata',\n",
       " 'ahymatana',\n",
       " 'ahymatanajwa',\n",
       " 'ai',\n",
       " 'aib',\n",
       " 'aid',\n",
       " 'ainunnajib',\n",
       " 'air',\n",
       " 'airmatabuaya',\n",
       " 'aiuorzpul',\n",
       " 'aj',\n",
       " 'aja',\n",
       " 'ajaib',\n",
       " 'ajak',\n",
       " 'ajalah',\n",
       " 'ajar',\n",
       " 'ajaran',\n",
       " 'ajarin',\n",
       " 'ajengcute',\n",
       " 'ajuin',\n",
       " 'ak',\n",
       " 'akal',\n",
       " 'akan',\n",
       " 'akbar',\n",
       " 'akh',\n",
       " 'akhir',\n",
       " 'akhirnya',\n",
       " 'akibat',\n",
       " 'akidah',\n",
       " 'akoydj',\n",
       " 'akrobat',\n",
       " 'aksi',\n",
       " 'aktif',\n",
       " 'aktivis',\n",
       " 'aktual',\n",
       " 'aku',\n",
       " 'akui',\n",
       " 'akun',\n",
       " 'akurat',\n",
       " 'al',\n",
       " 'alasan',\n",
       " 'alay',\n",
       " 'albaghdadi',\n",
       " 'albaghdi',\n",
       " 'alexemeru',\n",
       " 'alfauzannn',\n",
       " 'alfred',\n",
       " 'ali',\n",
       " 'alias',\n",
       " 'all',\n",
       " 'allah',\n",
       " 'alonkii',\n",
       " 'alquran',\n",
       " 'alternatif',\n",
       " 'alur',\n",
       " 'alvano',\n",
       " 'am',\n",
       " 'amarah',\n",
       " 'ambisinya',\n",
       " 'amerika',\n",
       " 'amin',\n",
       " 'amp',\n",
       " 'amplop',\n",
       " 'amysungkono',\n",
       " 'an',\n",
       " 'anak',\n",
       " 'anaknya',\n",
       " 'analisa',\n",
       " 'analisis',\n",
       " 'anas',\n",
       " 'ancam',\n",
       " 'ancaman',\n",
       " 'ancol',\n",
       " 'and',\n",
       " 'anda',\n",
       " 'andalah',\n",
       " 'andfadel',\n",
       " 'andiarief',\n",
       " 'andra',\n",
       " 'andreobz',\n",
       " 'android',\n",
       " 'aneh',\n",
       " 'anehh',\n",
       " 'anezbolowbolow',\n",
       " 'ang',\n",
       " 'angan',\n",
       " 'angel',\n",
       " 'anggara',\n",
       " 'anggaran',\n",
       " 'anggota',\n",
       " 'anggran',\n",
       " 'anggun',\n",
       " 'angke',\n",
       " 'angkot',\n",
       " 'anhu',\n",
       " 'ani',\n",
       " 'anies',\n",
       " 'aniesbaswedan',\n",
       " 'aniesberbohong',\n",
       " 'aniessandimenang',\n",
       " 'aniessandiokoce',\n",
       " 'anime',\n",
       " 'anis',\n",
       " 'anivsge',\n",
       " 'anjargrt',\n",
       " 'anjiirrr',\n",
       " 'anjir',\n",
       " 'anjrot',\n",
       " 'ank',\n",
       " 'annisapohan',\n",
       " 'annisapohon',\n",
       " 'antara',\n",
       " 'antasari',\n",
       " 'anti',\n",
       " 'anton',\n",
       " 'antrian',\n",
       " 'ap',\n",
       " 'apa',\n",
       " 'apaan',\n",
       " 'apaansi',\n",
       " 'apakah',\n",
       " 'apapun',\n",
       " 'apegmf',\n",
       " 'api',\n",
       " 'apik',\n",
       " 'apr',\n",
       " 'apresiasi',\n",
       " 'aqkcvqhy',\n",
       " 'arab',\n",
       " 'arah',\n",
       " 'arahkan',\n",
       " 'argumen',\n",
       " 'argumentasi',\n",
       " 'ariearie',\n",
       " 'ariel',\n",
       " 'arieskradianto',\n",
       " 'ariesya',\n",
       " 'arifahelfishy',\n",
       " 'arisan',\n",
       " 'arse',\n",
       " 'arswwoiudf',\n",
       " 'artikel',\n",
       " 'artinya',\n",
       " 'as',\n",
       " 'asal',\n",
       " 'asam',\n",
       " 'asep',\n",
       " 'asfairus',\n",
       " 'asiknya',\n",
       " 'asing',\n",
       " 'asli',\n",
       " 'aslinya',\n",
       " 'asshmaab',\n",
       " 'astaghfirullah',\n",
       " 'aswhyshoanx',\n",
       " 'at',\n",
       " 'atas',\n",
       " 'atasan',\n",
       " 'atasi',\n",
       " 'atlet',\n",
       " 'ato',\n",
       " 'atp',\n",
       " 'attack',\n",
       " 'attitude',\n",
       " 'aturan',\n",
       " 'aug',\n",
       " 'auliazein',\n",
       " 'aun',\n",
       " 'av',\n",
       " 'avlps',\n",
       " 'awal',\n",
       " 'awam',\n",
       " 'awas',\n",
       " 'awasi',\n",
       " 'awasituhoax',\n",
       " 'awc',\n",
       " 'awogxgvsfy',\n",
       " 'awww',\n",
       " 'axhdupfuvs',\n",
       " 'axm',\n",
       " 'axp',\n",
       " 'ay',\n",
       " 'ayahnya',\n",
       " 'ayam',\n",
       " 'ayat',\n",
       " 'ayo',\n",
       " 'ayusekarqq',\n",
       " 'azhar',\n",
       " 'azn',\n",
       " 'aztfzx',\n",
       " 'azwar',\n",
       " 'azx',\n",
       " 'babak',\n",
       " 'babihokers',\n",
       " 'babihox',\n",
       " 'baca',\n",
       " 'background',\n",
       " 'backup',\n",
       " 'bad',\n",
       " 'badan',\n",
       " 'badja',\n",
       " 'bagaikan',\n",
       " 'bagaimana',\n",
       " 'bagdadi',\n",
       " 'baghdadi',\n",
       " 'bagian',\n",
       " 'bagu',\n",
       " 'bagus',\n",
       " 'bagusnya',\n",
       " 'bahagia',\n",
       " 'bahagian',\n",
       " 'bahaya',\n",
       " 'baik',\n",
       " 'baiklah',\n",
       " 'baiknya',\n",
       " 'baikrt',\n",
       " 'baju',\n",
       " 'bajunya',\n",
       " 'bakal',\n",
       " 'balai',\n",
       " 'balaikota',\n",
       " 'balak',\n",
       " 'balas',\n",
       " 'bali',\n",
       " 'balik',\n",
       " 'balon',\n",
       " 'bama',\n",
       " 'bambang',\n",
       " 'bamburuncing',\n",
       " 'bamsbulaksumur',\n",
       " 'band',\n",
       " 'bang',\n",
       " 'banget',\n",
       " 'bangettt',\n",
       " 'bangga',\n",
       " 'bangka',\n",
       " 'bangkeeeeee',\n",
       " 'bangkru',\n",
       " 'bangndon',\n",
       " 'bangsa',\n",
       " 'bangun',\n",
       " 'bangzul',\n",
       " 'banjir',\n",
       " 'banner',\n",
       " 'banners',\n",
       " 'bansos',\n",
       " 'bantaeng',\n",
       " 'bantah',\n",
       " 'bantaran',\n",
       " 'bantuan',\n",
       " 'bantuin',\n",
       " 'banyak',\n",
       " 'banyaknya',\n",
       " 'bapak',\n",
       " 'bapaknya',\n",
       " 'barak',\n",
       " 'barat',\n",
       " 'bareng',\n",
       " 'bareskrim',\n",
       " 'barisan',\n",
       " 'baru',\n",
       " 'basuki',\n",
       " 'basukidjarot',\n",
       " 'baswedan',\n",
       " 'batack',\n",
       " 'batalion',\n",
       " 'batik',\n",
       " 'batunya',\n",
       " 'bawa',\n",
       " 'bawaannya',\n",
       " 'bawah',\n",
       " 'bawahan',\n",
       " 'bayangan',\n",
       " 'bayangin',\n",
       " 'bayaran',\n",
       " 'bb',\n",
       " 'bbsr',\n",
       " 'bbz',\n",
       " 'bclyoz',\n",
       " 'bcs',\n",
       " 'bdb',\n",
       " 'be',\n",
       " 'bebas',\n",
       " 'bebeb',\n",
       " 'beberapa',\n",
       " 'becah',\n",
       " 'becak',\n",
       " 'beda',\n",
       " 'bedakan',\n",
       " 'bedanya',\n",
       " 'bedjo',\n",
       " 'begini',\n",
       " 'beginian',\n",
       " 'begins',\n",
       " 'begono',\n",
       " 'bejanawaktu',\n",
       " 'bekerja',\n",
       " 'bela',\n",
       " 'belah',\n",
       " 'belajar',\n",
       " 'belanjaan',\n",
       " 'beli',\n",
       " 'beliau',\n",
       " 'belitung',\n",
       " 'belok',\n",
       " 'belom',\n",
       " 'belongs',\n",
       " 'belum',\n",
       " 'ben',\n",
       " 'benar',\n",
       " 'benarkah',\n",
       " 'benderang',\n",
       " 'bener',\n",
       " 'beneran',\n",
       " 'benernya',\n",
       " 'benerr',\n",
       " 'bentar',\n",
       " 'benteng',\n",
       " 'bentrok',\n",
       " 'bentur',\n",
       " 'benturbudaya',\n",
       " 'ber',\n",
       " 'berada',\n",
       " 'beragam',\n",
       " 'berakhlak',\n",
       " 'berandai',\n",
       " 'berangkat',\n",
       " 'berani',\n",
       " 'berantakan',\n",
       " 'berapa',\n",
       " 'berar',\n",
       " 'berasa',\n",
       " 'berawal',\n",
       " 'berbagai',\n",
       " 'berbahaya',\n",
       " 'berbicara',\n",
       " 'berbohong',\n",
       " 'berbuat',\n",
       " 'berdasar',\n",
       " 'berdebat',\n",
       " 'berdua',\n",
       " 'beredar',\n",
       " 'berentiin',\n",
       " 'beres',\n",
       " 'berfantasi',\n",
       " 'berfikir',\n",
       " 'bergelar',\n",
       " 'berhamburan',\n",
       " 'berharap',\n",
       " 'berhasil',\n",
       " 'berhenti',\n",
       " 'beri',\n",
       " 'berimajinasi',\n",
       " 'berita',\n",
       " 'beritariau',\n",
       " 'beritasatu',\n",
       " 'beritaterkini',\n",
       " 'berjalan',\n",
       " 'berjaya',\n",
       " 'berjenis',\n",
       " 'berju',\n",
       " 'berkarisma',\n",
       " 'berkarya',\n",
       " 'berkatifitas',\n",
       " 'berkempen',\n",
       " 'berkhayal',\n",
       " 'berkunjung',\n",
       " 'berlagak',\n",
       " 'berlebihan',\n",
       " 'bermasalah',\n",
       " 'bermimpi',\n",
       " 'bernyanyi',\n",
       " 'berolah',\n",
       " 'berpasangan',\n",
       " 'berpendidikan',\n",
       " 'berpesta',\n",
       " 'berpijak',\n",
       " 'berpikir',\n",
       " 'berpofesi',\n",
       " 'berpolitik',\n",
       " 'berprasangka',\n",
       " 'bers',\n",
       " 'bersabar',\n",
       " 'bersajak',\n",
       " 'bersalah',\n",
       " 'bersama',\n",
       " 'berselisih',\n",
       " 'berseragam',\n",
       " 'bersih',\n",
       " 'bertahan',\n",
       " 'bertanya',\n",
       " 'bertaruh',\n",
       " 'bertemu',\n",
       " 'bertengkar',\n",
       " 'berteriak',\n",
       " 'berubah',\n",
       " 'berujung',\n",
       " 'berupa',\n",
       " 'berwibawa',\n",
       " 'berwujud',\n",
       " 'besar',\n",
       " 'besi',\n",
       " 'besok',\n",
       " 'best',\n",
       " 'betapa',\n",
       " 'betawi',\n",
       " 'betul',\n",
       " 'bg',\n",
       " 'bgmn',\n",
       " 'bgr',\n",
       " 'bgt',\n",
       " 'bhnuotwvyr',\n",
       " 'bhw',\n",
       " 'biar',\n",
       " 'biarkan',\n",
       " 'biasa',\n",
       " 'bibir',\n",
       " 'bicara',\n",
       " 'bicaranya',\n",
       " 'bida',\n",
       " 'bidang',\n",
       " 'bijak',\n",
       " 'bikin',\n",
       " 'biko',\n",
       " 'bila',\n",
       " 'bilang',\n",
       " 'billykompas',\n",
       " 'bin',\n",
       " 'bingar',\n",
       " 'bingung',\n",
       " 'birokrasi',\n",
       " 'bisa',\n",
       " 'bisaaa',\n",
       " 'bisu',\n",
       " 'bjcblfmfnr',\n",
       " 'bjot',\n",
       " 'bjrj',\n",
       " 'bkasi',\n",
       " 'bkddjtx',\n",
       " 'bkfxocwbvr',\n",
       " 'bkn',\n",
       " 'bla',\n",
       " 'blak',\n",
       " 'blakan',\n",
       " 'blasphemy',\n",
       " 'bleach',\n",
       " 'blkang',\n",
       " 'blm',\n",
       " 'blogspot',\n",
       " 'blokir',\n",
       " 'blom',\n",
       " 'blt',\n",
       " 'blusukan',\n",
       " 'bm',\n",
       " 'bnanews',\n",
       " 'bngrwjo',\n",
       " 'bnujsvpi',\n",
       " 'bo',\n",
       " 'bobrok',\n",
       " 'bodoh',\n",
       " 'boehpeb',\n",
       " 'bohong',\n",
       " 'bohongi',\n",
       " 'bohongin',\n",
       " 'bokapnya',\n",
       " 'bokccb',\n",
       " 'boleh',\n",
       " 'bongkar',\n",
       " 'bonus',\n",
       " 'border',\n",
       " 'bos',\n",
       " 'bosnya',\n",
       " 'boss',\n",
       " 'bosss',\n",
       " 'boy',\n",
       " 'bozx',\n",
       " 'bp',\n",
       " 'bpk',\n",
       " 'bps',\n",
       " 'bpunr',\n",
       " 'bqgjr',\n",
       " 'bqmiogbagh',\n",
       " 'bqoyfzem',\n",
       " 'br',\n",
       " 'brain',\n",
       " 'brarti',\n",
       " 'bray',\n",
       " 'brigadaregion',\n",
       " 'bringas',\n",
       " 'brita',\n",
       " 'brivbgf',\n",
       " 'brkacak',\n",
       " 'brkemampuan',\n",
       " 'brkutik',\n",
       " 'bro',\n",
       " 'brsalah',\n",
       " 'brsama',\n",
       " 'brslh',\n",
       " 'bru',\n",
       " 'brutal',\n",
       " 'bs',\n",
       " 'bsa',\n",
       " 'bsp',\n",
       " 'bszm',\n",
       " 'btiqpe',\n",
       " 'btp',\n",
       " 'bu',\n",
       " 'buahhatinya',\n",
       " 'buang',\n",
       " 'buat',\n",
       " 'budget',\n",
       " 'budiman',\n",
       " 'budimandjatmiko',\n",
       " 'buka',\n",
       " 'bukaan',\n",
       " 'bukan',\n",
       " 'bukankah',\n",
       " 'bukannya',\n",
       " 'buknnya',\n",
       " 'bukti',\n",
       " 'buktikan',\n",
       " 'bul',\n",
       " 'bulkibalkibul',\n",
       " 'bullfront',\n",
       " 'bully',\n",
       " 'bulu',\n",
       " 'bumerang',\n",
       " 'bumi',\n",
       " 'bung',\n",
       " 'bungkam',\n",
       " 'bungsu',\n",
       " 'bungtomsss',\n",
       " 'buni',\n",
       " 'buniyani',\n",
       " 'bunuh',\n",
       " 'bunyi',\n",
       " 'buruk',\n",
       " 'burung',\n",
       " 'but',\n",
       " 'butuh',\n",
       " 'buya',\n",
       " 'buzzer',\n",
       " 'bvm',\n",
       " 'bwby',\n",
       " 'bwjy',\n",
       " 'by',\n",
       " 'byangannya',\n",
       " 'byk',\n",
       " 'bz',\n",
       " 'bzdkhwciem',\n",
       " 'bzux',\n",
       " 'ca',\n",
       " 'cac',\n",
       " 'caca',\n",
       " 'cadasss',\n",
       " 'cagu',\n",
       " 'cagub',\n",
       " 'cagubnya',\n",
       " 'cagubnyinyir',\n",
       " 'cahrjv',\n",
       " 'cai',\n",
       " 'cakep',\n",
       " 'calgub',\n",
       " 'call',\n",
       " 'calm',\n",
       " 'calon',\n",
       " 'calonin',\n",
       " 'camkan',\n",
       " 'campaign',\n",
       " 'can',\n",
       " 'cantik',\n",
       " 'caootefbtp',\n",
       " 'cape',\n",
       " 'capedehhhh',\n",
       " 'capres',\n",
       " 'cara',\n",
       " 'caranya',\n",
       " 'cari',\n",
       " 'case',\n",
       " 'cawagub',\n",
       " 'cb',\n",
       " 'cbse',\n",
       " 'cc',\n",
       " 'ccb',\n",
       " 'cdompu',\n",
       " 'cebong',\n",
       " 'cedm',\n",
       " 'cegah',\n",
       " 'cek',\n",
       " 'celah',\n",
       " 'celjudqc',\n",
       " 'celotehprabu',\n",
       " 'cendana',\n",
       " 'cerdas',\n",
       " 'cergtyhak',\n",
       " 'cerita',\n",
       " 'cermin',\n",
       " 'ceroboh',\n",
       " 'cewek',\n",
       " 'cf',\n",
       " 'cfavmozyix',\n",
       " 'cgz',\n",
       " 'chandra',\n",
       " 'cherrybelle',\n",
       " 'cherrybelleindo',\n",
       " 'china',\n",
       " 'chirpified',\n",
       " 'chkhz',\n",
       " 'chocoaca',\n",
       " 'chocoliq',\n",
       " 'chose',\n",
       " 'chuksjrntz',\n",
       " 'chy',\n",
       " 'cicakterbang',\n",
       " 'cikal',\n",
       " 'cincin',\n",
       " 'cipinang',\n",
       " 'citra',\n",
       " 'cjertypdxp',\n",
       " 'ckckck',\n",
       " 'ckgwh',\n",
       " 'ckita',\n",
       " 'ckup',\n",
       " 'ckwikzj',\n",
       " 'cldn',\n",
       " 'cm',\n",
       " 'cma',\n",
       " 'cmllyuikmz',\n",
       " 'cmn',\n",
       " 'cna',\n",
       " 'cnamanya',\n",
       " 'cndw',\n",
       " 'co',\n",
       " 'coba',\n",
       " 'cobain',\n",
       " 'coblos',\n",
       " 'coblosanissandi',\n",
       " 'coblosnomor',\n",
       " 'coblospecinya',\n",
       " 'cocok',\n",
       " 'color',\n",
       " 'com',\n",
       " 'coming',\n",
       " 'cont',\n",
       " 'contekanya',\n",
       " 'continues',\n",
       " 'contoh',\n",
       " 'contributors',\n",
       " 'coordinates',\n",
       " 'copot',\n",
       " 'count',\n",
       " 'counter',\n",
       " 'cowo',\n",
       " 'cowok',\n",
       " 'cproses',\n",
       " 'cqoilx',\n",
       " 'created',\n",
       " 'cries',\n",
       " 'criminal',\n",
       " 'crop',\n",
       " 'crrwcx',\n",
       " 'cs',\n",
       " 'csz',\n",
       " 'ct',\n",
       " 'ctc',\n",
       " 'cubxqvpxvp',\n",
       " 'cuci',\n",
       " 'cuef',\n",
       " 'cugf',\n",
       " 'cuirrblt',\n",
       " 'cujkjhvb',\n",
       " 'cuka',\n",
       " 'cukong',\n",
       " 'cukup',\n",
       " 'cuma',\n",
       " 'cumarachel',\n",
       " 'cupyoppaak',\n",
       " 'cuti',\n",
       " 'cv',\n",
       " 'cvh',\n",
       " 'cvtw',\n",
       " 'cwhkk',\n",
       " 'cwuh',\n",
       " 'cwy',\n",
       " 'cwzv',\n",
       " 'cx',\n",
       " 'cxectjknz',\n",
       " 'cxwmlthbxy',\n",
       " 'cz',\n",
       " 'da',\n",
       " 'dadansuwarna',\n",
       " 'daerah',\n",
       " 'daftar',\n",
       " 'dag',\n",
       " 'dah',\n",
       " 'dahhh',\n",
       " 'dajal',\n",
       " 'dalam',\n",
       " 'dalem',\n",
       " 'dana',\n",
       " 'danramil',\n",
       " 'dapet',\n",
       " 'dapitdong',\n",
       " 'dapur',\n",
       " 'darah',\n",
       " 'daritadi',\n",
       " 'darwistriadi',\n",
       " 'dasar',\n",
       " 'dasarcalonkarbitan',\n",
       " 'data',\n",
       " 'datang',\n",
       " 'datanya',\n",
       " 'day',\n",
       " 'dbanding',\n",
       " 'dcopot',\n",
       " 'dcxmfkrh',\n",
       " 'ddeef',\n",
       " 'dduvvxek',\n",
       " 'de',\n",
       " 'deba',\n",
       " 'debat',\n",
       " 'debatahypalingoke',\n",
       " 'debatcagubdki',\n",
       " 'debatnya',\n",
       " 'dec',\n",
       " 'december',\n",
       " 'deddy',\n",
       " 'deed',\n",
       " 'deelestari',\n",
       " 'default',\n",
       " 'deh',\n",
       " 'dekat',\n",
       " 'deklarasi',\n",
       " 'deklarasikan',\n",
       " 'demo',\n",
       " 'demokrasi',\n",
       " 'demokrat',\n",
       " 'dempf',\n",
       " 'dendam',\n",
       " 'dengar',\n",
       " 'denger',\n",
       " 'dengki',\n",
       " 'dennyja',\n",
       " 'densus',\n",
       " 'depan',\n",
       " 'deplayhafiezd',\n",
       " 'desak',\n",
       " 'descendant',\n",
       " 'description',\n",
       " 'desta',\n",
       " 'desvronitaaa',\n",
       " 'detikcom',\n",
       " 'dewa',\n",
       " 'dewasalah',\n",
       " 'df',\n",
       " 'dfojxl',\n",
       " 'dg',\n",
       " 'dgln',\n",
       " 'dgn',\n",
       " 'dgv',\n",
       " 'dha',\n",
       " 'dhani',\n",
       " 'dhnfdcu',\n",
       " 'dholimi',\n",
       " 'di',\n",
       " 'dia',\n",
       " 'diadili',\n",
       " 'diadu',\n",
       " 'diakui',\n",
       " 'dialog',\n",
       " 'diam',\n",
       " 'diambil',\n",
       " 'dianggap',\n",
       " 'diansyah',\n",
       " 'diare',\n",
       " 'diatas',\n",
       " 'diawal',\n",
       " 'diawasi',\n",
       " 'dibaca',\n",
       " 'dibalik',\n",
       " 'dibanding',\n",
       " 'dibangun',\n",
       " 'dibanyakin',\n",
       " 'dibayar',\n",
       " 'dibeda',\n",
       " 'dibela',\n",
       " 'dibelah',\n",
       " 'dibentak',\n",
       " 'dibentuk',\n",
       " 'diberi',\n",
       " 'dibiarkan',\n",
       " 'dibidang',\n",
       " 'dibikin',\n",
       " 'dibilang',\n",
       " 'dibodohi',\n",
       " 'dibohongi',\n",
       " 'dibuat',\n",
       " 'dibutuhkan',\n",
       " 'dicalonkan',\n",
       " 'diciptakan',\n",
       " 'didebat',\n",
       " 'didepan',\n",
       " 'didienazhar',\n",
       " 'didikte',\n",
       " 'didominasi',\n",
       " 'diduga',\n",
       " 'didukung',\n",
       " 'diene',\n",
       " 'dig',\n",
       " 'digawe',\n",
       " 'digebuk',\n",
       " 'digesek',\n",
       " 'dihadiri',\n",
       " 'dihasilkan',\n",
       " 'dihati',\n",
       " 'dihitamkan',\n",
       " 'dihujat',\n",
       " 'dii',\n",
       " 'diimbangi',\n",
       " 'dijadikan',\n",
       " 'dijalanin',\n",
       " 'dijalankan',\n",
       " 'dijatuhi',\n",
       " 'dijatuhin',\n",
       " 'dijawab',\n",
       " 'dijelaskan',\n",
       " 'dijual',\n",
       " 'dik',\n",
       " 'dikagumi',\n",
       " 'dikanan',\n",
       " 'dikasih',\n",
       " 'dikatakan',\n",
       " 'dikecilin',\n",
       " 'dikerjain',\n",
       " 'diketawain',\n",
       " 'diki',\n",
       " 'dikiri',\n",
       " 'dikit',\n",
       " 'dikomen',\n",
       " 'dikritik',\n",
       " 'dikritisi',\n",
       " 'dilakukan',\n",
       " 'dilawan',\n",
       " 'diledek',\n",
       " 'dilihat',\n",
       " 'diluar',\n",
       " 'dilukis',\n",
       " 'diluruskan',\n",
       " 'dimaafkan',\n",
       " 'dimas',\n",
       " 'dimasprakbar',\n",
       " 'dimengerti',\n",
       " 'diminta',\n",
       " 'dimuliakan',\n",
       " 'dinas',\n",
       " 'dinasti',\n",
       " 'dipakainya',\n",
       " 'dipake',\n",
       " 'dipaksakan',\n",
       " 'dipatahkan',\n",
       " 'dipati',\n",
       " 'dipecat',\n",
       " 'dipelihara',\n",
       " 'diperjelas',\n",
       " 'diperpanjang',\n",
       " 'dipilih',\n",
       " 'dipimpin',\n",
       " 'dipimpinnya',\n",
       " 'dipojokkan',\n",
       " 'dipolitisir',\n",
       " 'dipuja',\n",
       " 'dipungkiri',\n",
       " 'dira',\n",
       " 'dirajam',\n",
       " 'diralestari',\n",
       " 'dirangkul',\n",
       " 'dirasakan',\n",
       " 'direbut',\n",
       " 'direkrut',\n",
       " 'diri',\n",
       " 'dirinya',\n",
       " 'dirs',\n",
       " 'disamain',\n",
       " 'disebut',\n",
       " 'disebutkan',\n",
       " 'disemprot',\n",
       " 'disentuh',\n",
       " 'disiplin',\n",
       " 'disitu',\n",
       " 'diskotik',\n",
       " 'disoraki',\n",
       " 'display',\n",
       " 'disuruh',\n",
       " 'ditakuti',\n",
       " 'ditambah',\n",
       " 'ditangkap',\n",
       " 'ditanya',\n",
       " 'ditanyakan',\n",
       " 'ditawarkan',\n",
       " 'ditebak',\n",
       " 'ditekan',\n",
       " 'ditelan',\n",
       " 'ditempel',\n",
       " 'ditengah',\n",
       " 'ditindak',\n",
       " 'ditipu',\n",
       " 'ditonton',\n",
       " 'ditraktir',\n",
       " 'dituduh',\n",
       " 'ditugaskan',\n",
       " 'ditutup',\n",
       " 'diundang',\n",
       " 'diusung',\n",
       " 'divonis',\n",
       " 'dj',\n",
       " 'djakarta',\n",
       " 'djan',\n",
       " 'djarot',\n",
       " 'djeo',\n",
       " 'djpzxp',\n",
       " ...]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "cv = CountVectorizer()\n",
    "\n",
    "review_data_tf = cv.fit_transform(review_data.Tweet)\n",
    "cv.get_feature_names()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0, 0, 0, ..., 0, 0, 0],\n",
       "       [0, 0, 0, ..., 0, 0, 0],\n",
       "       [0, 0, 0, ..., 0, 0, 0],\n",
       "       ...,\n",
       "       [0, 0, 0, ..., 0, 0, 0],\n",
       "       [0, 0, 0, ..., 0, 0, 0],\n",
       "       [0, 0, 0, ..., 0, 0, 0]])"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "review_data_tf.toarray()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before we create a model, we have to split our data into two disjoint sets, train set and test set. Train set would be trained into classifier resulting a model. And then we can evaluate our model with test set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "trainX, testX, trainY, testY = train_test_split(review_data_tf, review_data.sentimen)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0, 0, 0, ..., 0, 0, 0],\n",
       "       [0, 0, 0, ..., 0, 0, 0],\n",
       "       [0, 0, 0, ..., 0, 0, 0],\n",
       "       ...,\n",
       "       [0, 0, 0, ..., 0, 0, 0],\n",
       "       [0, 0, 0, ..., 0, 0, 0],\n",
       "       [0, 0, 0, ..., 0, 0, 1]])"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainX.toarray()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0, 0, 0, ..., 0, 0, 0],\n",
       "       [0, 0, 0, ..., 0, 0, 0],\n",
       "       [0, 0, 0, ..., 0, 0, 0],\n",
       "       ...,\n",
       "       [0, 0, 0, ..., 0, 0, 0],\n",
       "       [0, 0, 0, ..., 0, 0, 0],\n",
       "       [0, 0, 0, ..., 0, 0, 0]])"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "testX.toarray()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[1,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " ...]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainY.tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[1,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 0]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "testY.tolist()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The next step is creating a model. The model can be achieve by training a classifier using the training data. One of the most suitable classifiers for text analysis is MultinomialNB. MultinomialNB is a variant of Naive Bayes classifier."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "MultinomialNB(alpha=1.0, class_prior=None, fit_prior=True)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.naive_bayes import MultinomialNB\n",
    "\n",
    "mnb = MultinomialNB()\n",
    "mnb.fit(trainX, trainY)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The accuracy of model can be achieved by feeding test set into trained model. Compare the result with the actual class of test set, then visualized it using confusion matrix. A confusion matrix is a technique for summarizing the performance of a classification algorithm."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = mnb.predict(testX)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[149,  43],\n",
       "       [ 41, 144]])"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.metrics import confusion_matrix, precision_recall_fscore_support, f1_score\n",
    "\n",
    "confusion_matrix(y_true=testY, y_pred=y_pred, labels=review_data.sentimen.unique())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "precision, recall, _, _ = precision_recall_fscore_support(y_true=testY, y_pred=y_pred, labels=review_data.sentimen.unique())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.78421053, 0.77005348])"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "precision"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.77604167, 0.77837838])"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "recall"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.7801047120418849"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "f1_score(y_true=testY, y_pred=y_pred, labels=review_data.sentimen.unique())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Predict sentiment of tweets crawled in \"Crawling Twitter Using Tweepy\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Tweet</th>\n",
       "      <th>Sentiment</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Ayo input data sebelum tanggal Maret Etam suks...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>OdedMD kangyanamulyana KHamidipradja NurShomad...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>ASN DPMPTSP NAKER Melakukan Proses pengisian s...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Selamat malam Sehubungan sedang dilaksanakanny...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>persen penduduk Kota Bogor lakukan sensus mand...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>690</th>\n",
       "      <td>Sosialisasi sensus penduduk online acara sambu...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>691</th>\n",
       "      <td>BABINSA KORAMIL BDK HADIRI RAPAT KOORDINASI KE...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>692</th>\n",
       "      <td>LOGIN https t co cnQuhbjWSm Cara Isi Sensus Pe...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>693</th>\n",
       "      <td>Rapat Koordinasi Sensus Penduduk Kegiatan dira...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>694</th>\n",
       "      <td>Dalam rangka meningkatkan keterlibatan Perguru...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>695 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                 Tweet  Sentiment\n",
       "0    Ayo input data sebelum tanggal Maret Etam suks...          0\n",
       "1    OdedMD kangyanamulyana KHamidipradja NurShomad...          1\n",
       "2    ASN DPMPTSP NAKER Melakukan Proses pengisian s...          1\n",
       "3    Selamat malam Sehubungan sedang dilaksanakanny...          1\n",
       "4    persen penduduk Kota Bogor lakukan sensus mand...          1\n",
       "..                                                 ...        ...\n",
       "690  Sosialisasi sensus penduduk online acara sambu...          1\n",
       "691  BABINSA KORAMIL BDK HADIRI RAPAT KOORDINASI KE...          1\n",
       "692  LOGIN https t co cnQuhbjWSm Cara Isi Sensus Pe...          0\n",
       "693  Rapat Koordinasi Sensus Penduduk Kegiatan dira...          0\n",
       "694  Dalam rangka meningkatkan keterlibatan Perguru...          0\n",
       "\n",
       "[695 rows x 2 columns]"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "f = open('sensus_penduduk_tweets.txt')\n",
    "tweets = f.readlines()\n",
    "\n",
    "# Tokenize\n",
    "tweets = list(map(lambda x: tokenizer.tokenize(x), tweets))\n",
    "tweets = list(map(lambda x: ' '.join(x), tweets))\n",
    "\n",
    "# Remove stop words\n",
    "tweets = list(map(lambda x: stopword.remove(x),tweets))\n",
    "\n",
    "# Predict sentiment\n",
    "tweets_tf = cv.transform(tweets)\n",
    "tweets_sentiments = mnb.predict(tweets_tf)\n",
    "\n",
    "# Show as table\n",
    "sensus_penduduk_sentiment = pd.DataFrame({'Tweet': tweets, 'Sentiment': tweets_sentiments})\n",
    "sensus_penduduk_sentiment"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Predict sentiment of tweets crawled in \"Crawling Web Using Selenium Webdriver\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Tweet</th>\n",
       "      <th>Sentiment</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Jumat Maret Cari Network Login Tribun Home Nas...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Jumat Maret Cari Network Login Tribun Home Nas...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>UNTUK INDONESIA BERITA OPINI CERITA PROFIL BOL...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Bahasa Indonesia English Profil Ragam Layanan ...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>MENU CARI JUM AT MARET Beranda Makro Nasional ...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>100</th>\n",
       "      <td>Skip to content Pencarian MARET HOME NASIONAL ...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>101</th>\n",
       "      <td>Login Buku Tambahkan KoleksikuTulis resensi Ha...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>102</th>\n",
       "      <td>JUMAT MARET WIB HOME NEWS LINGKUNGAN POLITIK H...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>103</th>\n",
       "      <td>LINE Today TOP Trending Showbiz News Life Regi...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>104</th>\n",
       "      <td>BERANDA HUKUM KRIMINAL PENDIDIKAN PEMERINTAHAN...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>105 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                 Tweet  Sentiment\n",
       "0    Jumat Maret Cari Network Login Tribun Home Nas...          1\n",
       "1    Jumat Maret Cari Network Login Tribun Home Nas...          1\n",
       "2    UNTUK INDONESIA BERITA OPINI CERITA PROFIL BOL...          1\n",
       "3    Bahasa Indonesia English Profil Ragam Layanan ...          0\n",
       "4    MENU CARI JUM AT MARET Beranda Makro Nasional ...          1\n",
       "..                                                 ...        ...\n",
       "100  Skip to content Pencarian MARET HOME NASIONAL ...          1\n",
       "101  Login Buku Tambahkan KoleksikuTulis resensi Ha...          1\n",
       "102  JUMAT MARET WIB HOME NEWS LINGKUNGAN POLITIK H...          1\n",
       "103  LINE Today TOP Trending Showbiz News Life Regi...          1\n",
       "104  BERANDA HUKUM KRIMINAL PENDIDIKAN PEMERINTAHAN...          1\n",
       "\n",
       "[105 rows x 2 columns]"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "f = open('sensus_penduduk_news.txt')\n",
    "news = f.readlines()\n",
    "\n",
    "# Tokenize\n",
    "news = list(map(lambda x: tokenizer.tokenize(x), news))\n",
    "news = list(map(lambda x: ' '.join(x), news))\n",
    "\n",
    "# Remove stop words\n",
    "news = list(map(lambda x: stopword.remove(x),news))\n",
    "\n",
    "# Predict sentiment\n",
    "news_tf = cv.transform(news)\n",
    "news_sentiments = mnb.predict(news_tf)\n",
    "\n",
    "# Show as table\n",
    "sensus_penduduk_sentiment = pd.DataFrame({'Tweet': news, 'Sentiment': news_sentiments})\n",
    "sensus_penduduk_sentiment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
